{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech command prediction with federated learning\n",
    "\n",
    "This is an example of federated leraning for audio data \n",
    "The objective of this model is to predict speech command correctly. \n",
    "\n",
    "I borrowed almost all codes from this repository. Thank a lot!  \n",
    "https://github.com/tugstugi/pytorch-speech-commands.git  \n",
    "We skip a few steps like WeightedRandomSampler and lr_scheduler for for simplicity\n",
    "\n",
    "Federated learning parts is taken from PySyft tutorial.  \n",
    "https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2006%20-%20Federated%20Learning%20on%20MNIST%20using%20a%20CNN.ipynb\n",
    "\n",
    "you can learn \n",
    "1. how to handle audio datasets\n",
    "2. how to apply federated learning concepts on audio datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fist let's do setup for jupyter note book\n",
    "\n",
    "# ignore warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "repository\n",
    "# some jupyter specific settings\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies.\n",
    "# mostly torch relatd\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "from tqdm import *\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, we need set default type as torch.cuda.FloatTensor\n",
    "# you get type error without this as of 9/1/2020\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's define tutorials objective here \n",
    "\n",
    "We're training a model which take audio wav file as input and output the index of speech commands.\n",
    "\n",
    "- Input: wav audio file\n",
    "- Output: index of speech commands\n",
    "\n",
    "So it's a classification problem.\n",
    "\n",
    "In this tutorial, we have 12 classes to predict.  \n",
    "unknown, silence, yes, no, up, down, left, right, on, off, stop, go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define classes here \n",
    "# CLASSES = 'unknown, silence, yes, no, up, down, left, right, on, off, stop, go'.split(', ')\n",
    "# use subset of classes\n",
    "CLASSES = 'unknown, silence, yes, no, left, right'.split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's prepare datasets\n",
    "\n",
    "we download speech command datasets from here  \n",
    "http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "\n",
    "# create directory if not exist\n",
    "# we put data on datasets directory\n",
    "if os.path.isdir('./datasets') is False:\n",
    "    try:\n",
    "        os.mkdir('./datasets')\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory datasets failed\")\n",
    "\n",
    "if os.path.isdir('./datasets/speech_commands') is True:\n",
    "    print(\"datasets seems to exists.\")\n",
    "else :\n",
    "    # download data\n",
    "    ! wget -O datasets/speech_commands_v0.01.tar.gz http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
    "    \n",
    "    # create directory\n",
    "    os.mkdir('./datasets/speech_commands')\n",
    "    \n",
    "    # create audio directory\n",
    "    if os.path.isdir('./datasets/speech_commands/audio') is False:\n",
    "        try:\n",
    "            os.mkdir('./datasets/speech_commands/audio')\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory datasets/speech_commands/audio failed\")\n",
    "        \n",
    "\n",
    "    # untar files.\n",
    "    ! tar -xzf datasets/speech_commands_v0.01.tar.gz -C datasets/speech_commands/audio      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once you downloaded datasets,\n",
    "# you can split datasets into training and validation datasets.\n",
    "# we split with csv file.\n",
    "\n",
    "# mode files \n",
    "def move_files(src_folder, to_folder, list_file):\n",
    "    with open(list_file) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.rstrip()\n",
    "            dirname = os.path.dirname(line)\n",
    "            dest = os.path.join(to_folder, dirname)\n",
    "            if not os.path.exists(dest):\n",
    "                os.mkdir(dest)\n",
    "            shutil.move(os.path.join(src_folder, line), dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move files\n",
    "def prepare_dataset():\n",
    "    audio_folder = \"datasets/speech_commands/audio\"\n",
    "    validation_path = \"datasets/speech_commands/audio/validation_list.txt\"\n",
    "    test_path = \"datasets/speech_commands/audio/testing_list.txt\"\n",
    "\n",
    "    valid_folder = \"datasets/speech_commands/valid\"\n",
    "    test_folder = \"datasets/speech_commands/test\"\n",
    "    train_folder = \"datasets/speech_commands/train\"\n",
    "\n",
    "    if os.path.isdir(valid_folder) is False:\n",
    "        os.mkdir(valid_folder)\n",
    "    if os.path.isdir(test_folder) is False:\n",
    "        os.mkdir(test_folder)\n",
    "\n",
    "    move_files(audio_folder, test_folder, test_path)\n",
    "    move_files(audio_folder, valid_folder, validation_path)\n",
    "    os.rename(audio_folder, train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "if os.path.isdir('./datasets/speech_commands/train') is False:\n",
    "    prepare_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check datasets\n",
    "\n",
    "Now we have datasets.\n",
    "Let's check one of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems like category is in the paths.\n",
    "# input data is wav audio file and output is index of \"right\"\n",
    "import IPython.display\n",
    "example_path = \"datasets/speech_commands/train/right/9f4098cb_nohash_0.wav\"\n",
    "\n",
    "IPython.display.Audio(example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define functions to process audio.\n",
    "# basically convert raw audio into stft and convert stft into mel spectrogram and do some augmentation.\n",
    "# poits is since we deal with audio like images in this tutorial, we need every audio exact 1 seconds.\n",
    "# I mean all audio has exact same duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just returning true or false ramdomly\n",
    "def should_apply_transform(prob=0.5):\n",
    "    \"\"\"Transforms are only randomly applied with the given probability.\"\"\"\n",
    "    return random.random() < prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change ampletude for data augmentation\n",
    "class ChangeAmplitude(object):\n",
    "    \"\"\"Changes amplitude of an audio randomly.\"\"\"\n",
    "\n",
    "    def __init__(self, amplitude_range=(0.7, 1.1)):\n",
    "        self.amplitude_range = amplitude_range\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if not should_apply_transform():\n",
    "            return data\n",
    "\n",
    "        data['samples'] = data['samples'] * random.uniform(*self.amplitude_range)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change speedch and pitch for data augmentation\n",
    "class ChangeSpeedAndPitchAudio(object):\n",
    "    \"\"\"Change the speed of an audio. This transform also changes the pitch of the audio.\"\"\"\n",
    "\n",
    "    def __init__(self, max_scale=0.2):\n",
    "        self.max_scale = max_scale\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if not should_apply_transform():\n",
    "            return data\n",
    "\n",
    "        samples = data['samples']\n",
    "        sample_rate = data['sample_rate']\n",
    "        scale = random.uniform(-self.max_scale, self.max_scale)\n",
    "        speed_fac = 1.0  / (1 + scale)\n",
    "        data['samples'] = np.interp(np.arange(0, len(samples), speed_fac), np.arange(0,len(samples)), samples).astype(np.float32)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fix audio length \n",
    "# Because our architecture is not RNN-based but CNN-based, we need shapes of all input data exact same.\n",
    "class FixAudioLength(object):\n",
    "    \"\"\"Either pads or truncates an audio into a fixed length.\"\"\"\n",
    "\n",
    "    def __init__(self, time=1):\n",
    "        self.time = time\n",
    "\n",
    "    def __call__(self, data):\n",
    "        samples = data['samples']\n",
    "        sample_rate = data['sample_rate']\n",
    "        length = int(self.time * sample_rate)\n",
    "        if length < len(samples):\n",
    "            data['samples'] = samples[:length]\n",
    "        elif length > len(samples):\n",
    "            data['samples'] = np.pad(samples, (0, length - len(samples)), \"constant\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert raw audio samping into stft \n",
    "class ToSTFT(object):\n",
    "    \"\"\"Applies on an audio the short time fourier transform.\"\"\"\n",
    "\n",
    "    def __init__(self, n_fft=2048, hop_length=512):\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def __call__(self, data):\n",
    "        samples = data['samples']\n",
    "        sample_rate = data['sample_rate']\n",
    "        data['n_fft'] = self.n_fft\n",
    "        data['hop_length'] = self.hop_length\n",
    "        data['stft'] = librosa.stft(samples, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        data['stft_shape'] = data['stft'].shape\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StretchAudioOnSTFT(object):\n",
    "    \"\"\"Stretches an audio on the frequency domain.\"\"\"\n",
    "\n",
    "    def __init__(self, max_scale=0.2):\n",
    "        self.max_scale = max_scale\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if not should_apply_transform():\n",
    "            return data\n",
    "\n",
    "        stft = data['stft']\n",
    "        sample_rate = data['sample_rate']\n",
    "        hop_length = data['hop_length']\n",
    "        scale = random.uniform(-self.max_scale, self.max_scale)\n",
    "        stft_stretch = librosa.core.phase_vocoder(stft, 1+scale, hop_length=hop_length)\n",
    "        data['stft'] = stft_stretch\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeshiftAudioOnSTFT(object):\n",
    "    \"\"\"A simple timeshift on the frequency domain without multiplying with exp.\"\"\"\n",
    "\n",
    "    def __init__(self, max_shift=8):\n",
    "        self.max_shift = max_shift\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if not should_apply_transform():\n",
    "            return data\n",
    "\n",
    "        stft = data['stft']\n",
    "        shift = random.randint(-self.max_shift, self.max_shift)\n",
    "        a = -min(0, shift)\n",
    "        b = max(0, shift)\n",
    "        stft = np.pad(stft, ((0, 0), (a, b)), \"constant\")\n",
    "        if a == 0:\n",
    "            stft = stft[:,b:]\n",
    "        else:\n",
    "            stft = stft[:,0:-a]\n",
    "        data['stft'] = stft\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixSTFTDimension(object):\n",
    "    \"\"\"Either pads or truncates in the time axis on the frequency domain, applied after stretching, time shifting etc.\"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        stft = data['stft']\n",
    "        t_len = stft.shape[1]\n",
    "        orig_t_len = data['stft_shape'][1]\n",
    "        if t_len > orig_t_len:\n",
    "            stft = stft[:,0:orig_t_len]\n",
    "        elif t_len < orig_t_len:\n",
    "            stft = np.pad(stft, ((0, 0), (0, orig_t_len-t_len)), \"constant\")\n",
    "\n",
    "        data['stft'] = stft\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we dedfine data augmentatio functions \n",
    "\n",
    "data_aug_transform = Compose([\n",
    "    ChangeAmplitude(), \n",
    "    ChangeSpeedAndPitchAudio(), \n",
    "    FixAudioLength(), \n",
    "    ToSTFT(), \n",
    "    StretchAudioOnSTFT(), \n",
    "    TimeshiftAudioOnSTFT(), \n",
    "    FixSTFTDimension()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Noise Augmentation\n",
    "Adding background noise on the fly is a good way to generalize audio model.    \n",
    "we send dataset to remote machines (workers) later with fedelted() command and it seems to be working.  \n",
    "But I haven't tested addig noise augmentation on real remote training settings.  \n",
    "\n",
    "What if noise file does not exist on remote machine???   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define a way to add background noise\n",
    "class BackgroundNoiseDataset(Dataset):\n",
    "    \"\"\"Dataset for silence / background noise.\"\"\"\n",
    "\n",
    "    def __init__(self, folder, transform=None, sample_rate=16000, sample_length=1):\n",
    "        audio_files = [d for d in os.listdir(folder) if os.path.isfile(os.path.join(folder, d)) and d.endswith('.wav')]\n",
    "        samples = []\n",
    "        for f in audio_files:\n",
    "            path = os.path.join(folder, f)\n",
    "            s, sr = librosa.load(path, sample_rate)\n",
    "            samples.append(s)\n",
    "\n",
    "        samples = np.hstack(samples)\n",
    "        c = int(sample_rate * sample_length)\n",
    "        r = len(samples) // c\n",
    "        self.samples = samples[:r*c].reshape(-1, c)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.classes = CLASSES\n",
    "        self.transform = transform\n",
    "        self.path = folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {'samples': self.samples[index], 'sample_rate': self.sample_rate, 'target': 1, 'path': self.path}\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we pick background noise randomly. so use dataset class\n",
    "background_noise_dir = \"./datasets/speech_commands/train/_background_noise_\"\n",
    "bg_dataset = BackgroundNoiseDataset(background_noise_dir, data_aug_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add background noise on datasets\n",
    "class AddBackgroundNoiseOnSTFT(Dataset):\n",
    "    \"\"\"Adds a random background noise on the frequency domain.\"\"\"\n",
    "\n",
    "    def __init__(self, bg_dataset, max_percentage=0.45):\n",
    "        self.bg_dataset = bg_dataset\n",
    "        self.max_percentage = max_percentage\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if not should_apply_transform():\n",
    "            return data\n",
    "\n",
    "        noise = random.choice(self.bg_dataset)['stft']\n",
    "        percentage = random.uniform(0, self.max_percentage)\n",
    "        data['stft'] = data['stft'] * (1 - percentage) + noise * percentage\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function\n",
    "add_bg_noise = AddBackgroundNoiseOnSTFT(bg_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MelSpectrogram\n",
    "\n",
    "In this tutorial we use mel spectrogram as input format.    \n",
    "Since our data format is still stft so far , this is the time to convert stft into mel spectrogram.  \n",
    "\n",
    "mel spectrogram is one of best practices to handle audio data.  \n",
    "This blog explains mel spectrogram well.  \n",
    "https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert data from STFT into MelSpectrogram\n",
    "\n",
    "class ToMelSpectrogramFromSTFT(object):\n",
    "    \"\"\"Creates the mel spectrogram from the short time fourier transform of a file. The result is a 32x32 matrix.\"\"\"\n",
    "\n",
    "    def __init__(self, n_mels=32):\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "    def __call__(self, data):\n",
    "        stft = data['stft']\n",
    "        sample_rate = data['sample_rate']\n",
    "        n_fft = data['n_fft']\n",
    "        mel_basis = librosa.filters.mel(sample_rate, n_fft, self.n_mels)\n",
    "        s = np.dot(mel_basis, np.abs(stft)**2.0)\n",
    "        data['mel_spectrogram'] = librosa.power_to_db(s, ref=np.max)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeleteSTFT(object):\n",
    "    \"\"\"Pytorch doesn't like complex numbers, use this transform to remove STFT after computing the mel spectrogram.\"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        del data['stft']\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Converts into a tensor.\"\"\"\n",
    "\n",
    "    def __init__(self, np_name, tensor_name, normalize=None):\n",
    "        self.np_name = np_name\n",
    "        self.tensor_name = tensor_name\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, data):\n",
    "        tensor = torch.FloatTensor(data[self.np_name])\n",
    "        if self.normalize is not None:\n",
    "            mean, std = self.normalize\n",
    "            tensor -= mean\n",
    "            tensor /= std\n",
    "        data[self.tensor_name] = tensor\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a couple of functions to one\n",
    "\n",
    "# set the feature count of mel spectrogram as 32.\n",
    "n_mels = 32\n",
    "\n",
    "train_feature_transform = Compose([\n",
    "    ToMelSpectrogramFromSTFT(n_mels=n_mels), \n",
    "    DeleteSTFT(), \n",
    "    ToTensor('mel_spectrogram', 'input')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "\n",
    "So far...\n",
    "- We download data.\n",
    "- split data into training and valdation.\n",
    "- define data augmentation\n",
    "- define adding noise daga augmentation\n",
    "- define functions to convert raw audio to stft format\n",
    "- define funcitons to convert stft format into mel spectrogram format.\n",
    "\n",
    "lets use above to define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load audio.\n",
    "class LoadAudio(object):\n",
    "    \"\"\"Loads an audio into a numpy array.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \n",
    "        path = data['path']\n",
    "        if path:\n",
    "            samples, sample_rate = librosa.load(path, self.sample_rate)\n",
    "        else:\n",
    "            # silence\n",
    "            sample_rate = self.sample_rate\n",
    "            samples = np.zeros(sample_rate, dtype=np.float32)\n",
    "        data['samples'] = samples\n",
    "        data['sample_rate'] = sample_rate\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets class\n",
    "# you can try subset of entire datasets with use_rate because distributing datasets to remote machines take time...\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "class SpeechCommandsDataset(Dataset):\n",
    "    \"\"\"Google speech commands dataset. Only 'yes', 'no', 'up', 'down', 'left',\n",
    "    'right', 'on', 'off', 'stop' and 'go' are treated as known classes.\n",
    "    All other classes are used as 'unknown' samples.\n",
    "    See for more information: https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, folder, transform=None, classes=CLASSES, silence_percentage=0.1, use_rate=1.0):\n",
    "        all_classes = [d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder, d)) and not d.startswith('_')]\n",
    "        #for c in classes[2:]:\n",
    "        #    assert c in all_classes\n",
    "\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        for c in all_classes:\n",
    "            if c not in class_to_idx:\n",
    "                class_to_idx[c] = 0\n",
    "        \n",
    "        # we use subset of datasets\n",
    "        data = []\n",
    "        for c in all_classes:\n",
    "            d = os.path.join(folder, c)\n",
    "            target = class_to_idx[c]\n",
    "            for f in os.listdir(d):\n",
    "                path = os.path.join(d, f)\n",
    "                data.append((path, target))\n",
    "        \n",
    "        shuffle(data)\n",
    "        if use_rate != 1.0:\n",
    "            sample_count = int(len(data) * use_rate)\n",
    "            data = data[:sample_count]\n",
    "        \n",
    "\n",
    "        # add silence\n",
    "        target = class_to_idx['silence']\n",
    "        data += [('', target)] * int(len(data) * silence_percentage)\n",
    "\n",
    "        self.classes = classes\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.data[index]\n",
    "        data = {'path': path, 'target': target}\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data['input'], target\n",
    "\n",
    "    def make_weights_for_balanced_classes(self):\n",
    "        \"\"\"adopted from https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3\"\"\"\n",
    "\n",
    "        nclasses = len(self.classes)\n",
    "        count = np.zeros(nclasses)\n",
    "        for item in self.data:\n",
    "            count[item[1]] += 1\n",
    "\n",
    "        N = float(sum(count))\n",
    "        weight_per_class = N / count\n",
    "        weight = np.zeros(len(self))\n",
    "        for idx, item in enumerate(self.data):\n",
    "            weight[idx] = weight_per_class[item[1]]\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we define dataset here\n",
    "\n",
    "# specify the percent of entire datasets\n",
    "# use_rate = 0.2\n",
    "use_rate = 1.0\n",
    "\n",
    "train_dataset_dir = \"./datasets/speech_commands/train\"\n",
    "train_dataset = SpeechCommandsDataset(train_dataset_dir,\n",
    "                                Compose([LoadAudio(),\n",
    "                                         data_aug_transform,\n",
    "                                         add_bg_noise,\n",
    "                                         train_feature_transform]), use_rate=use_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data count\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function to create melSpectrogram datasets from audio directly to skip data augumentation\n",
    "# this is used in validation data\n",
    "class ToMelSpectrogram(object):\n",
    "    \"\"\"Creates the mel spectrogram from an audio. The result is a 32x32 matrix.\"\"\"\n",
    "\n",
    "    def __init__(self, n_mels=32):\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "    def __call__(self, data):\n",
    "        samples = data['samples']\n",
    "        sample_rate = data['sample_rate']\n",
    "        s = librosa.feature.melspectrogram(samples, sr=sample_rate, n_mels=self.n_mels)\n",
    "        data['mel_spectrogram'] = librosa.power_to_db(s, ref=np.max)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_feature_transform = Compose([\n",
    "    ToMelSpectrogram(n_mels=n_mels), \n",
    "    ToTensor('mel_spectrogram', 'input')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validation datasets\n",
    "\n",
    "valid_dataset_dir = \"./datasets/speech_commands/valid\"\n",
    "valid_dataset = SpeechCommandsDataset(valid_dataset_dir,\n",
    "                                Compose([LoadAudio(),\n",
    "                                         FixAudioLength(),\n",
    "                                         valid_feature_transform]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "Finally we can apply federated learning here.  \n",
    "Validation datasets is just normal but we use PySyft library to split training dataset into 2 machines (workers) called Bob and Alice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloaders\n",
    "\n",
    "# batch size is 64\n",
    "batch_size = 64\n",
    "\n",
    "# we define training dataloader later right after importing PySyft, library for privacy preserving deep learning\n",
    "\n",
    "# define validation dataloader, which is just normal dataloader\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for federated leaning\n",
    "In this tutorial we traing our model with federated learning.\n",
    "To do that, we do \n",
    "- import syft\n",
    "- create 2 machines (workders) called bob and alice \n",
    "- split training datasets and send them to bob and alice \n",
    "\n",
    "Note: In real business situation, each machines should have data in the first place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaine federated dataloader\n",
    "# it takes time. be patient.\n",
    "federated_train_loader = sy.FederatedDataLoader(\n",
    "    train_dataset.federate((bob, alice))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "we define architecture here.  \n",
    "We use ResNet34 in this tutorial.  \n",
    "ResNet is one of popular architecture for image problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define conv block\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define res block\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ResNet\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, in_channels=3):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(1, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resnet34\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet34-333f7ec4.pth'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model \n",
    "model = resnet34(num_classes=len(CLASSES), in_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device setting.\n",
    "# pleas use gpu, this example is too heavy for cpu training.\n",
    "\n",
    "# if use_gpu:\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to gpu if you use gpu\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function is normal crossentrophy\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from syft.federated.floptimizer import Optims\n",
    "\n",
    "# define optimizer\n",
    "# adamw seems to be better\n",
    "learning_rate = 1e-4\n",
    "# weight_decay = 1e-2\n",
    "\n",
    "# from syft.federated.floptimizer import Optims\n",
    "# workers = ['bob', 'alice']\n",
    "# optims = Optims(workers, optim=torch.optim.SGD(params=model.parameters(), lr=learning_rate ))\n",
    "\n",
    "from syft.federated.floptimizer import Optims\n",
    "workers = ['bob', 'alice']\n",
    "optims = Optims(workers, optim=torch.optim.AdamW(params=model.parameters(), lr=learning_rate ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timestamp = int(time.time()*1000)\n",
    "start_epoch = 0\n",
    "# max_epochs = 30\n",
    "max_epochs = 5\n",
    "best_accuracy = 0\n",
    "best_loss = 1e100\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you want to fine-tune model, make finetune True\n",
    "# finetune = False\n",
    "\n",
    "# if finetune is True:\n",
    "#     # load saved weights\n",
    "#     weight_path = \"./checkpoints/best-acc-speech-commands-checkpoint-basic1.pth\"\n",
    "#     state = torch.load(\n",
    "#         weight_path, \n",
    "#         map_location=torch.device(\"cpu\"))\n",
    "#     _ = model.load_state_dict(state2['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trining loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_name = \"speech_command_with_fl\"\n",
    "\n",
    "def train(epoch):\n",
    "    global global_step\n",
    "\n",
    "    # print(\"epoch %3d with lr=%.02e\" % (epoch, get_lr()))\n",
    "    phase = 'train'\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # pbar = tqdm(train_dataloader, unit=\"audios\", unit_scale=train_dataloader.batch_size)\n",
    "    \n",
    "    # use federated_train_loader\n",
    "    pbar = tqdm(federated_train_loader, unit=\"audios\", unit_scale=batch_size)\n",
    "    for batch in pbar:\n",
    "        \n",
    "        inputs = batch[0]\n",
    "        targets = batch[1]\n",
    "        \n",
    "        # get optimaizer on the same location with data\n",
    "        _optimizer = optims.get_optim(inputs.location.id)\n",
    "        \n",
    "        # reset grad\n",
    "        _optimizer.zero_grad()\n",
    "        \n",
    "        # send model to data.location\n",
    "        model.send(inputs.location)\n",
    "        \n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward/backward\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # get loss \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # backward and step \n",
    "        loss.backward()\n",
    "        _optimizer.step()\n",
    "        \n",
    "        # get model back\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        \n",
    "        # get loss back\n",
    "        loss = loss.get() # <-- NEW: get the loss back\n",
    "\n",
    "        # statistics\n",
    "        it += 1\n",
    "        global_step += 1\n",
    "    \n",
    "        # running_loss += loss.data[0]\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pred = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # keep is to statistics\n",
    "        _correct = pred.eq(targets.view_as(pred)).sum()\n",
    "        correct += _correct.get().item()\n",
    "        total += targets.shape[0]\n",
    "        \n",
    "        # update the progress bar    \n",
    "        pbar.set_postfix({\n",
    "            'loss': \"%.05f\" % (running_loss / it),\n",
    "            'acc': \"%.02f%%\" % (100*correct/total)\n",
    "        })\n",
    "    \n",
    "    accuracy = correct/total\n",
    "    epoch_loss = running_loss / it\n",
    "    print('%s/accuracy' % phase, 100*accuracy, epoch)\n",
    "    print('%s/epoch_loss' % phase, epoch_loss, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(epoch):\n",
    "    global best_accuracy, best_loss, global_step\n",
    "    \n",
    "    phase = 'valid'\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in valid_dataloader:\n",
    "         \n",
    "        inputs = batch[0]\n",
    "          \n",
    "        targets = batch[1]\n",
    "        \n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "          \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # statistics\n",
    "        it += 1\n",
    "        global_step += 1\n",
    "        running_loss += loss.item()\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "\n",
    "        _correct = pred.eq(targets.view_as(pred)).sum().item()\n",
    "        correct += _correct\n",
    "\n",
    "        total += targets.size(0)\n",
    "        \n",
    "    accuracy = correct/total\n",
    "    epoch_loss = running_loss / it\n",
    "    \n",
    "    print('%s/accuracy' % phase, 100*accuracy, epoch)\n",
    "    print('%s/epoch_loss' % phase, epoch_loss, epoch)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'step': global_step,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'optimizer': optims.get_optim(bob.id).state_dict(),\n",
    "    }\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(checkpoint, 'checkpoints/best-loss-speech-commands-checkpoint-%s.pth' % full_name)\n",
    "        torch.save(model, '%d-%s-best-loss.pth' % (start_timestamp, full_name))\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(checkpoint, 'checkpoints/best-acc-speech-commands-checkpoint-%s.pth' % full_name)\n",
    "        torch.save(model, '%d-%s-best-acc.pth' % (start_timestamp, full_name))\n",
    "    \n",
    "    torch.save(checkpoint, 'checkpoints/last-speech-commands-checkpoint.pth')\n",
    "    del checkpoint  # reduce memory\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_epoch = 0\n",
    "\n",
    "if os.path.isdir('./checkpoints') is False:\n",
    "    try:\n",
    "        os.mkdir('./checkpoints')\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    \n",
    "\n",
    "since = time.time()\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "\n",
    "    train(epoch)\n",
    "    epoch_loss = valid(epoch)\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    time_str = 'total time elapsed: {:.0f}h {:.0f}m {:.0f}s '.format(time_elapsed // 3600, time_elapsed % 3600 // 60, time_elapsed % 60)\n",
    "    \n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set default type torch.FloatTensor)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see a data sample\n",
    "\n",
    "import IPython.display\n",
    "\n",
    "example_path = \"./datasets/speech_commands/train/right/9f4098cb_nohash_0.wav\"\n",
    "IPython.display.Audio(example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model again\n",
    "model2 = resnet34(num_classes=len(CLASSES), in_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved weights\n",
    "# weight_path2 = \"./checkpoints/best-acc-speech-commands-checkpoint-basic1.pth\"\n",
    "weight_path2 = \"./checkpoints/best-acc-speech-commands-checkpoint-speech_command_with_fl.pth\"\n",
    "\n",
    "state2 = torch.load(\n",
    "    weight_path2, \n",
    "    map_location=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model2.load_state_dict(state2['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio \n",
    "# this is exact steps to load validation datasets\n",
    "\n",
    "# our load audio function need this format\n",
    "sample_data = {\n",
    "    'path': example_path,\n",
    "}\n",
    "\n",
    "# load audio \n",
    "_load_audio = LoadAudio()\n",
    "_audio_sample = _load_audio(sample_data)\n",
    "len(_audio_sample[\"samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix duration\n",
    "_fixAudioLength = FixAudioLength()\n",
    "_fixed_sample = _fixAudioLength(_audio_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply validation transform\n",
    "_processed_input = valid_feature_transform(_fixed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust shape\n",
    "_processed_input = _processed_input['input'][None][None]\n",
    "_processed_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "_ = model2.eval()\n",
    "_eval_output = model2(_processed_input)\n",
    "_eval_pred = _eval_output.max(1, keepdim=True)[1]\n",
    "_eval_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
